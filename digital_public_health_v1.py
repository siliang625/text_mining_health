# -*- coding: utf-8 -*-
"""digital_public_health_v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10X1NtiqBkiFJKilGWuHbWKwHrK8ntHbc

# Problem Statement

In the notebook, we attemped to test the functionality of key word search and extracting full content and metadata given an article. 

In an exmaple below, we are able to 1) use key word 'public health' to find a list of related article ids, and 2) extracting these articles' titles
"""

!pip install BeautifulSoup4
!pip install lxml

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sys
np.set_printoptions(threshold=80, edgeitems=50)
from bs4 import BeautifulSoup
from collections import namedtuple
import codecs
import re
import time
import json
import requests
from urllib.request import urlopen

"""# key word search
we will use 'public health' as a sample search term. Sample query: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pmc&term=public%20health
"""

base_url_key_word = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pmc&term=public%20health"

page_kw = urlopen(base_url_key_word)
soup_kw = BeautifulSoup(page_kw, "xml")

# get text form of everything in the xml file
soup_kw.get_text()

"""The query can only return 20 related article ids as a upper limit, the actural amount of related articles is way bigger (eg: 1960797)"""

# total ids in the database
soup_kw.find('Count').text

"""Let's get a list of related article ids return by the query:"""

id_list = soup_kw.find_all('Id')
return_ids = []
for each_id in id_list:
    print(each_id.text)
    return_ids.append(each_id.text)

"""How 'public health' is queried in the databse? Based on the query results below, it uses 3 terms ('public', 'health', 'public health') and combine the search result.
```
# This is formatted as code
<TranslationStack>
<TermSet>
<Term>"public health"[MeSH Terms]</Term>
<Field>MeSH Terms</Field>
<Count>1186646</Count>
<Explode>Y</Explode>
</TermSet>
<TermSet>
<Term>"public"[All Fields]</Term>
<Field>All Fields</Field>
<Count>1397349</Count>
<Explode>N</Explode>
</TermSet>
<TermSet>
<Term>"health"[All Fields]</Term>
<Field>All Fields</Field>
<Count>3262233</Count>
<Explode>N</Explode>
</TermSet>
<OP>AND</OP>
<OP>GROUP</OP>
<OP>OR</OP>
<TermSet>
<Term>"public health"[All Fields]</Term>
<Field>All Fields</Field>
<Count>969981</Count>
<Explode>N</Explode>
</TermSet>
<OP>OR</OP>
<OP>GROUP</OP>
</TranslationStack>
<QueryTranslation>
"public health"[MeSH Terms] OR ("public"[All Fields] AND "health"[All Fields]) OR "public health"[All Fields]
</QueryTranslation>
```

# full text search
The sample query provided below aloows as to extract the full text of the articles, as well as some of the metadata (eg: author, abstract, reference, etc.)

Sample query: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=7225297

Let's extract the full content of the sample xml form
"""

base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id="

pmc_id = "7225297"

target_url = base_url + pmc_id

page = urlopen(target_url)
soup = BeautifulSoup(page, "xml")

# get text form of everything in the xml file
soup.get_text()

"""Now, let's extract some metadata form the xml

### article content
"""

contrib = soup.find('body')
paragraphs = contrib.find_all('p')
for p in paragraphs:
    print(p.text)

"""### Title"""

title = soup.find('title-group')
article_title = title.find('article-title')
print(article_title.text)

"""### Abstract"""

soup.find('abstract').text

"""### Author"""

contrib = soup.find('contrib-group')
authors = contrib.find_all('contrib', **{'contrib-type':"author"})
for author in authors:
    name = author.find('name')
    print(' '.join([author.find('surname').text, author.find('given-names').text]))

"""Now, let's say we want to have the name of all articles related to term 'public health':"""

mapping = {}
base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id="
for each in return_ids:
    target_url = base_url + each
    page = urlopen(target_url)
    soup = BeautifulSoup(page, "xml")
    title = soup.find('title-group')
    article_title = title.find('article-title')
    #print(article_title.text)
    mapping[each] = article_title.text
    time.sleep(1)
    
mapping

"""# ID converter api 

One academic article might have several ids: 

1. PMID: use simple numbers, e.g., 23193287.
2. PMCID: include the ‘PMC’ prefix, e.g., PMC3531190. You may drop the prefix if you select the checkbox for ‘Process as PMCIDs’.
3. Manuscript ID: include the relevant prefix, e.g., NIHMS236863 or EMS48932.
4. DOI: enter the complete string, e.g., 10.1093/nar/gks1195.

Fortunately, ncbi provided a service to convert ids between them. Let's try it out
"""

base_url_converter = "https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/"

sample_pmid = "23193287"

api_url = base_url_converter + "?ids=" + sample_pmid + "&format=json"
#print(api_url)
response = requests.get(api_url)

if response.status_code == 200:
  res = json.loads(response.content.decode('utf-8'))
  #print(res)
  print("Corresponding pmcid is: {}".format(res['records'][0]['pmcid']))
else:
  print("error")

"""# Conclusion:
- Looks like are able to use key word search to find all detailed information of the related articles(content, metadata, etc.)
- We need to set a sleeping time between each http call, even for 20 conecutive calls :c


### TODO
- how to solve the amount of ids return by the key word search?
- encapsulating and refactoring code
- future article content processing

# Reference

[1] doc of beautiful soup library: https://beautiful-soup-4.readthedocs.io/en/latest/index.html?highlight=find#find

[2] ID converver: https://www.ncbi.nlm.nih.gov/pmc/tools/id-converter-api/

[3] another way to extract metadata in json form: https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=32448042
"""

